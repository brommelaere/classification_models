---
title: "Measuring Performance of Classification Models"
output: html_notebook
---
```{r setup, include = FALSE}
library(tidyverse)
library(ggplot2)
```

When modelling classification problems, most models explicitly predict the probability of class membership. This is very important to keep in mind as we can use these predicted probabilities to come up with alternative classification rules. Sometimes these probabilities are directly useful for the problem at hand; for example, when calculating credit value at risk we calculate the probability of default and multiply it by the total value of the outstanding loan.

Not all models generate predicted class probabilities. For example, neural networks and partial least square produce continous predictions that do not follow the rules of probabilities.^[The value must lay on the unit line, between 0 and 1, and all probabilities must sum to 1.] For these models, we can coerce the predicted value to follow the mathamatical rules of probabilities so that they can be interpreted and used in classification tasks.^[An example of this is the softmax transformation as in Bridle, 1990.] 

## Calibrated Probabilities
When working with prediction probabilities, our theortical goal is to have the predicted class probability reflect the true sample probability that the given obersvation will be a member of the class. That is, our goal is to have our predicted class probabilty be well-calibrated. One way to assess the quality of class probabilities is to use a *calibration plot*. For a given set of data, this plot shows some measure of the observed probability of an event versus the predicted class probability.

ENTER CALIBRATION PLOT EXAMPLE WITH CODE

If the calibration plot shows that one model has poor calibration probabilties, they can be adjusted for. One such approach is to use a logistic regression model to post-process the prability estimates following this formula: $$\hat{p}^{calibrated} = \frac{1}{1 + e^{-\beta_0 + \beta_1\hat{p}^{model}}}$$ Alternatively, we could use an application of the Bayes' Rule to recalibrate the predictions.^[After recalibration, the sample must be reclassified to ensure consistency between our newly calibrated probabilities and the predicted class label]

ENTER BAYES RULE. ENTER EXAMPLE OF BAYES AND LOGISTIC RECALIBRATION CODE. 

## Presentation of Class Probabilities
Visualizing the class probabilties allow us to effectively communicate the model results and to illustrate the strengths and weaknesses of a given model. One way to present class probabilites is to present a *histogram of the test set probabilities for each of the two true classes*. If we have more than two classes, we could alternatively use a *heat map* to present class probabilities. 

## Equivocal Zones
We can improve classification performance by creatign equivocal or intedeterminate zones where the class is not predicted when the probability falls within some range of the prespcified threshold we are using for classification. For examplpe, we could create an equivocal zone of #0.50 \pm q#, where we would set #q# equal to some value such as #0.1#. Then everything in the range of 0.4 to 0.6 would be the equivocal zone and model performance would be calculated excluding the sample within this zone. 

# Evaluation Metrics
When evaluating mode performance of a classification task we can look at several measures to determine which model or specification predicts class membership most accurately. We can categorize these evaluation metrics into two sets: those that evaluate the predicted class and those that evaluate the predicted probability. 

## Predicted Classes
### The Confusion Matrix
The most common method for describing performance of a classification is a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). This amounts to a simple cross-tabulation of the observed and predicted classes. This gives us a simple count of the observations which are correctly predicted and our false-positive & false-negative rate. 

ENTER CONFUSION MATRIX EXAMPLE WITH LABELLING OF VALUES ALIGNING WITH NOTATION BELOW

### Simple Error Rate
The most straightfoward measure to interpret is the overall error rate of misspecifying class membership. However, this measure does not distinguish between the type of error which may be important for the business task at hand. For example, if you are predicting credit risk on P2P platform you are likely more worried about false-negatives; that is, those who you model predicts won't default but actually do. If you model has a high false-negative rate, you are putting more value at risk than if your model has a high false-positive rate. 

Errors rates are also misleading when you have class imbalance. For example, if only 0.2% of loan seekers default on the P2P lending platform then any model which predicts no default will be 99.8% accurate. This is obviously probematic. We can resolve by comparing our error rate to that of a no-information rate, which can simply be thought of as the percentage of observations in our test data belonging the relavent class. *Class imbalances* and their remidies are looked at in a later section. 

### Kappa Statistic 
The [Kappa statistic](https://www.sciencedirect.com/topics/medicine-and-dentistry/kappa-statistics) takes into account the class distribution of the training set by utilizing the accuracy one could achive by random chance. The statistic takes the form: $$\kappa = \frac{Observed\:Accuracy - Expected\:Accuracy}{1 - Expected\:Accuracy}$$ where expected accuracy is simply calculated using the marginal totals from our confusion matrix. That is we calculated expected accuracy as: 
$Expected\:Accuracy = \frac{[POV * PPV] + [NOV * NPV] }{n^2}$ where POV is the number of observed positive values, NOV is the number of observed negative values, PPV is the number of positive predicted values, and NPV is the number of negative predicted values. And we calculate observed accuracy as: $Observed\:Accuracy = \frac{n - FP - FN}{n}$ where FP is the number of false positives and FN is the number of false negatives. 

The Kappa statistic takes values between -1 & 1; a value of 0 means no agreement between observed and predicted classes, while a 1 would indicate that they perfectly align. This statistic can be extended to problems with two or more classes. However, Matthews Correlation Coefficient is perfered in multi-class classification problems as the Kappa statistic can produce contradictory behavour in multiclass classification problems. For more, see [here](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0222916)

### Matthews Correlation Coefficient
Another measure of prediction quaity for classification models is [Matthews correlation coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient). The coefficient directly takes the false-positive and false-negative rate into account. This measure treats the true class and the predicted class as two binary variables and computes the correlation coefficient between them: the higher the correlation between true and predicted values, the better the prediction. We can also calculate this metric directly from the confusion matrix and its marginal probabilties: $$MCC = \frac{(TP*TN)-(FP*FN)}{\sqrt{(TP + FP)(TP+FN)(TN+FP)(TN+FN)}}$$
It is straightforward to see that when the classifier is perfect, ie. FN=FP=0, the value of the MCC is 1, which can be read as eprfect positive correlation. Conversely, when the classifier is always wrong, MCC is -1. As this metric can be interpreted as a correlation coefficient, we can think of a 0 as implying that our model is no better than a random coin flip. 

One positive of MCC is that it is perfectly symmetric, meaning that non class is more important than the other. A high value of the MCC means that both classes are prediceted well, even if one class is disproportionately under represented (or over represented).

### Sensititivity & Specificity
Consider a problem with two generic classes: event and nonevent. In simple two class casses, there are additional statistics which may be of use when one of the classes is interpreted as an *event of interest*. These are called the *sensitivity* & *specificity* which measure two different, but interrelated model errors. 

The *sensitivity* of the model can be thought of as the true positive rate since it measure accuracy in the event population. It is defined as the rate that the event of interest is predicted correctly for all the observations where the event actually occurs:
$$Sensitivity = \frac{\#\;of\;observations\;with\;event\;\&\;predicted\;to\;have\;event}{\#\;of\;observations\;with\;event}$$
Conversely, *specificity* is defined as the rate that nonevents are predicted as nonevents. The false positive rate can be calculated as 1-*specificity*. Specificity is formally defined as: $$Specificity = \frac{\#\;of\;observations\;with\;nonevent\;\&\;predicted\;to\;have\;nonevent}{\#\;of\;observations\;with\;nonevent}$$

Intuitively, increasing the sensitivity of a model is likely to incur a loss of specificity, since more samples are being predicted as events. Potential trade-offs between sensitivity and specificity may be appropriate when there are different penalties associated with each type of error. The *receiver operating characteristic (ROC) curve* is one technique for evaluating this trade-off.

One often overlooked aspect of sensitivity and specificity is that they are conditional measures. Sensitivity is the accuracy rate for only the event population (and specificity for the nonevents). However, the person using the model prediction is typically interested in unconditional measures. Taking the *prevalence* into account, the analog to sensitivity is the *positive predicted value (PPV)*, and the analog to specificity is the *negative predicted value (NPV)*. The positive predicted value answers the question “what is the probability that this sample is an event?”

$$PPV = \frac{Sensititvity * Prevalence}{(Sensititvity * Prevalence) + ((1-Specificity)*(1-Prevalence))}$$
$$NPV = \frac{Sensititvity * (1-Prevalence)}{(Sensititvity * (1-Prevalence))+ (Specificity * (1-Prevalence))}$$
Clearly, the predictive values are nontrivial combinations of performance and the rate of events. Large negative predictive values can be achieved when the prevalence is low. However, as the event rate becomes high, the negative predictive value becomes very small. The opposite is true for the positive predictive values.

Predictive values are not often used to characterize the model. There are several reasons why, most of which are related to prevalence. Chiefly because prevalence is hard to quantify. Very few people would claim to know default rates of clientel based on prior knowledge alone.  Furthermore, prevalence may be dynamic, wherein more defaults may happen if new government policy is introduced, only to fall back to their baseline once the market goes back to equilirbium.

## Probability Cost Functions
We may no be solely interested in the accuracy of the predictive model for our busienss purpose, but may instead be more interested in maximizing our return or minimiznig our cost. This can be done with [probability cost curves](https://pdfs.semanticscholar.org/1cb2/7905d497f4e097c4f77f527eb77814ecf8d9.pdf). In the case of our default rate prediction case-study, we worry about default because it implies we lose money when a creditor defaults. In other words, when our model predicts the creditor will not default, but they do, a false-negative, then money is at risk. On the other hand, a false-positive is money we could have earned. Suppose we seek to maximize profits from investing \$100 in each of the creditors on our platform. In this case, if the creditor defaults, we will lose our entire investment of \$100, so this will be the cost of false-negatives. If our interest rate is 8%, then we expect to make \$8 from a successful investment. Then our profit function becomes: $profit = \$8TP-\$8FP-\$100FN$. From here, we can simply calculate the expected profit when we assume no default, and consider performance of above this baseline profit level. We can think of this as measuring *profit gain, or lift*, which is the profit above blindly investing in everyone. 

Here is a general outline for incorporating unequal costs with performance measures as suggested by [Drummond and Holte (2000)](https://pdfs.semanticscholar.org/1cb2/7905d497f4e097c4f77f527eb77814ecf8d9.pdf). First we dfine the *probability cost function (PCF)*: $$PCF = \frac{p*C_{event}}{p*C_{nonevent} + (1-p)*C_{event}}$$ where p is the probability of an event occuring in the data. The PCF measures the proportion of total costs associated with a false-positive observation. This allows to then use the *normalized expected cost (NEC) function* to characterize our model: $$NEC = PCF * (1-TP) + (1-PCF)*FP$$. Essentially, the NEC takes into account the prevalence of the event, model performance, and the costs and scales the total cost to be between 0 and 1. Note that this approach only assigns costs to the two types of errors and might not be appropriate for problems where there are other cost or benefits.

## Predicted Class Probabilities
Class probabilities offer more detailed information about our model predictions than the simple class values. The also can be used to assign new classification rules for our class predictions to improve accuracy, which is especially useful in imballanced classification tasks. We now look at the main approaches to using these probabilities to compare our models. 

### Receiver Operating Characteristic (ROC) Curve
ROC curves are a general method for determining thresholds that indicate where or not an event happened, and they can be used for determining alternate cutoffs for class probabilities. Instead of just predicting our class based on those with a greater than 50% chance are assigned to the default class, we can alter this threshold and the ROC gives us a tool to measure which is best. The ROC curve is created by evaluating the class probabilities for the model across a continuum of thresholds. For each candidate threshold, the resulting true-positive rate (i.e., the sensitivity) and the false-positive rate (one minus the specificity) are plotted against each other. This plot is a helpful tool for choosing a threshold that appropriately maximizes the trade-off between sensitivity and specificity. Note that, altering the threshold only has the effect of making samples more or less default predictions. In the confusion matrix, it cannot move samples out of both off-diagonal table cells. There is almost always a decrease in either sensitivity or specificity as 1 is increased. 

We can also use ROC curves to quantitatevly assess our models performance and compare different sets of features for the same model, different model tuning parameters, and different classifier algorithms altogether. A perfect model (which does not exist), would seperate the two classes perfectly and have 100% specificity and sensitivitiy. Graphically, the ROC curve would be a single step from (0,0) to (0,1) and would remain contant to (1,1). In this case the *area under the ROC curve* would be one. On the other hand, a completely ineffective model would have a ROC curve along the 45 degree diagonal and would have an *area under the ROC curve* equal to 0.5. Thus we can use the *area under the ROC curve* to directly compare model performance. We can also superimpose the ROC curves. 

One advantage of using ROC curves to characterize models is that, since it is a function of sensitivity and specificity, the curve is insensitive to disparities in the class proportions. A disadvantage of using the area under the curve to evaluate models is that it obscures information. This might be the case if we aren't interested in the whole area under the curve, but only a portion of it is important for the business case at hand.^[Which is the case for the creditor model as we have unequal costs.] 

The ROC curve is only technically defined for two-class problems, but has been extended to be able to handle [three or more classes](https://cran.r-project.org/web/packages/multiROC/multiROC.pdf). 

### Lift Charts
Lift charts are another visualization tool which allows us to assess the ability of the model to deterct events in the data. Lift charts are created by first sorting observations by the class probability. We then rank the observations by event class probability (score) and determine the cumulative event rate as we evaluate more observations. The idea is that we would expect that events are ranked higher than nonevents. In the optimal case, the N highest ranked samples would contain all N ovservations in the event class. When the model is not informative at all, the highest ranked X% of events would only contain, on average, X events. The *lift* is then the number of observations with events detected by the model above what we would get from completely the cumulative percentage of observations that have been screened.

To construct a *lift chart* we take the following steps:

1.  Predict a set of samples that were not used in the model building process but have known outcomes. 
2.  Determine the baseline event rate, i.e., the percent of true events in the entire data set.
3.  Order the data by the classification probability of the event of interest.
4.  For each unique class probability value, calculate the percent of true events in all samples below the probability value.
5.  Divide the percent of true events for each probability threshold by the baseline event rate.

The non-informative model has a curve that is close to the 45◦ reference line, meaning that the model has no benefit for ranking samples. Like with the ROC curve, an informative model bends towards the top-left corner. Like ROC curves, the lift curves for different models can be compared to find the most appropriate model and the area under the curve can be used as a quantitative measure of performance. Also like ROC curves, some parts of the lift curve are of more interest than others. In these cases, using the lift plot, the expected profit can be calculated for each point on the curve to determine if the lift is sufficient to beat the baseline profit.

